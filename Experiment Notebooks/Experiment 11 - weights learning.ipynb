{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "cuda = True\n",
    "epochs = 10\n",
    "log_interval = 400\n",
    "batchSize = 16\n",
    "\n",
    "hidden_layer_sizes = [3 * 32 * 32, 256, 10]\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input, axis=1):\n",
    "    input_size = input.size()\n",
    "    \n",
    "    trans_input = input.transpose(axis, len(input_size)-1)\n",
    "    trans_size = trans_input.size()\n",
    "\n",
    "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "    \n",
    "    soft_max_2d = F.softmax(input_2d)\n",
    "    \n",
    "    soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "    return soft_max_nd.transpose(axis, len(input_size)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.coefs = nn.Parameter(torch.FloatTensor(dim, dim))\n",
    "        self.coefs.data.normal_(0, 1.)\n",
    "        self.normal_coefs = torch.exp(-self.coefs) / torch.exp(-self.coefs).sum(1).repeat(1, dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.linear(X, weight=self.normal_coefs)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 16GB. Buy new RAM! at /py/conda-bld/pytorch_1493676237139/work/torch/lib/TH/THGeneral.c:270",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3790591d1a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3790591d1a40>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 16GB. Buy new RAM! at /py/conda-bld/pytorch_1493676237139/work/torch/lib/TH/THGeneral.c:270"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i+1]) for i in range(len(hidden_layer_sizes) - 1)])\n",
    "        self.attentions = nn.ModuleList([nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i] ** 2) for i in range(1, len(hidden_layer_sizes) - 1)])\n",
    "        for i, l in enumerate(self.attentions):\n",
    "            self.attentions[i].bias.data.copy_(torch.eye(self.attentions[i].weight.size(0)).view(-1))\n",
    "            scale = .1 / torch.sqrt(self.attentions[i].weight.size(0))\n",
    "            self.attentions[i].weight.data.uniform_(-scale,scale)\n",
    "    \n",
    "    def clamp(self, weight):\n",
    "        size = int(np.sqrt(weigh.size(0)))\n",
    "        weight = weight.view(size, size)\n",
    "        weight.data.clamp_(0., 1.)\n",
    "        weight.data.div_(self.attentions[i].sum(1).repeat(1, weight.size(0)).data)\n",
    "        return weight\n",
    "            \n",
    "    def forward(self, x):\n",
    "        activations = 0\n",
    "        x = x.view(-1, 3 * 32 * 32)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(l(x))\n",
    "            if i < len(self.attentions):\n",
    "                weights = self.clamp(self.attentions[i](torch.sign(x)))\n",
    "                x = F.linear(x, weight=weights)\n",
    "            if i + 2 == len(hidden_layer_sizes):\n",
    "                break\n",
    "            if type(activations) is np.ndarray:\n",
    "                activations = np.append(activations, np.expand_dims(torch.sign(x).cpu().data.numpy(), axis=1), axis=1)\n",
    "            else:\n",
    "                activations = np.expand_dims(torch.sign(x).cpu().data.numpy(), axis=1)\n",
    "        return x, activations\n",
    "    \n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD([\n",
    "                {'params': model.linears.parameters()},\n",
    "                {'params': model.attentions.parameters(), 'lr': 1e-2}\n",
    "            ], lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output, activations = model(data)\n",
    "        loss = criterion(output, target) + (epoch - 1.) / 10* 10000 * model.attentions[0].abs().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clamp()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.data[0]))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in testloader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output, activations = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(testloader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.278446\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.182377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 41, in _worker_loop\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    r = index_queue.get()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    return recv()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torchvision-0.1.8-py2.7.egg/torchvision/datasets/cifar.py\", line 96, in __getitem__\n",
      "    buf = self.recv_bytes()\n",
      "    img = Image.fromarray(img)\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 2313, in fromarray\n",
      "KeyboardInterrupt\n",
      "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 2266, in frombuffer\n",
      "    return frombytes(mode, size, data, decoder_name, args)\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 2198, in frombytes\n",
      "    im = new(mode, size)\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 2162, in new\n",
      "    return Image()._new(core.fill(mode, size, color))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ef6e9654c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-11399d7541a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# activation_metrics()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB0F6DjlOe0plAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4813333c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.attentions[0].sum(0).squeeze().cpu().data.numpy(), bins=np.arange(0., 2.5, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8848\n",
       " 0.5671\n",
       " 0.9629\n",
       "   â‹®   \n",
       " 1.7440\n",
       " 1.4152\n",
       " 0.6981\n",
       "[torch.cuda.FloatTensor of size 1024 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attentions[0].sum(0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

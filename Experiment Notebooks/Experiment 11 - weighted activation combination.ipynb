{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "cuda = True\n",
    "epochs = 10\n",
    "log_interval = 400\n",
    "batchSize = 16\n",
    "\n",
    "hidden_layer_sizes = [3 * 32 * 32, 256, 10]\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input, axis=1):\n",
    "    input_size = input.size()\n",
    "    \n",
    "    trans_input = input.transpose(axis, len(input_size)-1)\n",
    "    trans_size = trans_input.size()\n",
    "\n",
    "    input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "    \n",
    "    soft_max_2d = F.softmax(input_2d)\n",
    "    \n",
    "    soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "    return soft_max_nd.transpose(axis, len(input_size)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.coefs = nn.Parameter(torch.FloatTensor(dim, dim))\n",
    "        self.coefs.data.normal_(0, 1.)\n",
    "        self.normal_coefs = torch.exp(-self.coefs) / torch.exp(-self.coefs).sum(1).repeat(1, dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.linear(X, weight=self.normal_coefs)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (linears): ModuleList (\n",
      "    (0): Linear (3072 -> 256)\n",
      "    (1): Linear (256 -> 10)\n",
      "  )\n",
      "  (attentions): ParameterList (\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i+1]) for i in range(len(hidden_layer_sizes) - 1)])\n",
    "        self.attentions = nn.ParameterList([nn.Parameter(torch.FloatTensor(hidden_layer_sizes[i], hidden_layer_sizes[i])) for i in range(1, len(hidden_layer_sizes) - 1)])\n",
    "        for i, l in enumerate(self.attentions):\n",
    "            self.attentions[i].data.copy_(torch.eye(self.attentions[i].size(0)))\n",
    "    \n",
    "    def clamp(self):\n",
    "        for i, l in enumerate(self.attentions):\n",
    "            self.attentions[i].data.clamp_(0., 1.)\n",
    "            self.attentions[i].data.div_(self.attentions[i].sum(1).repeat(1, self.attentions[i].size(0)).data)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        activations = 0\n",
    "        x = x.view(-1, 3 * 32 * 32)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(l(x))\n",
    "            if i < len(self.attentions):\n",
    "                x = F.linear(x, weight=self.attentions[i])\n",
    "            if i + 2 == len(hidden_layer_sizes):\n",
    "                break\n",
    "            if type(activations) is np.ndarray:\n",
    "                activations = np.append(activations, np.expand_dims(torch.sign(x).cpu().data.numpy(), axis=1), axis=1)\n",
    "            else:\n",
    "                activations = np.expand_dims(torch.sign(x).cpu().data.numpy(), axis=1)\n",
    "        return x, activations\n",
    "    \n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD([\n",
    "                {'params': model.linears.parameters()},\n",
    "                {'params': model.attentions.parameters(), 'lr': 1e-2}\n",
    "            ], lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output, activations = model(data)\n",
    "        loss = criterion(output, target) + 1000 * model.attentions[0].abs().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clamp()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.data[0]))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in testloader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output, activations = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(testloader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))\n",
    "    \n",
    "def activation_metrics():\n",
    "    metrics = Constellation()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output, activations = model(data)\n",
    "        metrics.train_add(activations, target.cpu().data.numpy())\n",
    "        if batch_idx > 1000:\n",
    "            break\n",
    "#     for batch_idx, (data, target) in enumerate(testloader):\n",
    "#         if cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "#         data, target = Variable(data), Variable(target)\n",
    "#         output, activations = model(data)\n",
    "#         metrics.test_add(activations, target.cpu().data.numpy())\n",
    "        \n",
    "    metrics.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 6.208212\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 5.873016\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 5.925508\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 5.589938\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 5.639214\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 5.681391\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 5.785744\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 5.988539\n",
      "\n",
      "Test set: Average loss: 1.5744, Accuracy: 4443/10000 (44%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 5.381462\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 5.534988\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 5.191372\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 5.397735\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 5.711457\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 5.323517\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 5.644985\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 5.495381\n",
      "\n",
      "Test set: Average loss: 1.4638, Accuracy: 4922/10000 (49%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 5.138123\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 5.492385\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 5.739969\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 5.182818\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 5.166100\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 5.299533\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 5.410352\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 5.599227\n",
      "\n",
      "Test set: Average loss: 1.4223, Accuracy: 5020/10000 (50%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 4.963492\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 4.916486\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 5.007955\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 5.063539\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 5.191833\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 5.524002\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 5.106766\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 5.201001\n",
      "\n",
      "Test set: Average loss: 1.4007, Accuracy: 5104/10000 (51%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 5.159706\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 4.962847\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 5.442508\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 4.878228\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 4.996719\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 4.835426\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 5.464532\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 5.382793\n",
      "\n",
      "Test set: Average loss: 1.3555, Accuracy: 5273/10000 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 4.609302\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 5.154486\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 5.195621\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 4.910786\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 5.099443\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 4.944884\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 5.132418\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 5.607013\n",
      "\n",
      "Test set: Average loss: 1.3613, Accuracy: 5204/10000 (52%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 5.120075\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 5.224209\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 4.814120\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 5.368343\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 4.969815\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 4.717699\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 5.493929\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 4.930796\n",
      "\n",
      "Test set: Average loss: 1.3389, Accuracy: 5348/10000 (53%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 4.691375\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 4.989573\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 4.899722\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 5.161591\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 5.350540\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 4.782855\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 4.934018\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 5.442308\n",
      "\n",
      "Test set: Average loss: 1.3453, Accuracy: 5318/10000 (53%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 5.147419\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 4.952705\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 5.084330\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 4.821411\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 5.511299\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 4.851973\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 4.742262\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 5.043692\n",
      "\n",
      "Test set: Average loss: 1.3634, Accuracy: 5219/10000 (52%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 5.494049\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 4.521977\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 5.319181\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 5.197265\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 4.858377\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 4.970877\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 4.878761\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 5.533415\n",
      "\n",
      "Test set: Average loss: 1.3611, Accuracy: 5270/10000 (53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# activation_metrics()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-7223c999e09b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3079\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3082\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mhist\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   6277\u001b[0m                 patch = _barfunc(bins[:-1]+boffset, height, width,\n\u001b[1;32m   6278\u001b[0m                                  \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6279\u001b[0;31m                                  color=c, **{bottom_kwarg: bottom})\n\u001b[0m\u001b[1;32m   6280\u001b[0m                 \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6281\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, left, height, width, bottom, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m             \u001b[0mymin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintervaly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0mbar_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBarContainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lucas/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mautoscale_view\u001b[0;34m(self, tight, scalex, scaley)\u001b[0m\n\u001b[1;32m   2266\u001b[0m             \u001b[0mstickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msticky_edges\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0martist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m             \u001b[0mx_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msticky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msticky\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m             \u001b[0my_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msticky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msticky\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'log'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 \u001b[0mx_stickies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_stickies\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt.hist(model.attentions[0].squeeze().cpu().data.numpy(), bins=np.arange(0., 1., 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.32353318e-01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   9.91017044e-01,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   9.99983966e-01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.16491329e-03, ...,\n",
       "          9.46410537e-01,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  6.58738194e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   9.84207928e-01,   0.00000000e+00],\n",
       "       [  1.33649027e-03,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   9.70637798e-01]], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attentions[0].squeeze().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
